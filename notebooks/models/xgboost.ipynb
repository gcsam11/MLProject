{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.feature_selection import mutual_info_classif\n",
    "from sklearn.model_selection import StratifiedKFold, cross_validate\n",
    "from sklearn.metrics import make_scorer, confusion_matrix, roc_curve, auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the teams data\n",
    "dataset = pd.read_csv('../../cleaned_data/dataset.csv')\n",
    "\n",
    "label = 'PlayoffNextSeason'\n",
    "features = [\n",
    "    'CumulativePlayoffProgScore', '3P%', 'FT%',\n",
    "    'OFFRTG', 'DEFRTG', 'AST RATIO', 'REB%', 'TOV%',\n",
    "    'PACE', 'AvgPIE_NextYearPlayers', 'Performance_NextYearCoach'\n",
    "]\n",
    "\n",
    "# Prepare feature and label data\n",
    "X = dataset[features]\n",
    "y = dataset[label]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================\n",
    "# Display Initial Correlation Matrix\n",
    "# ============================\n",
    "\n",
    "# Calculate and plot the correlation matrix\n",
    "correlation_matrix = dataset[features + [label]].corr()\n",
    "plt.figure(figsize=(20, 18))\n",
    "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', center=0)\n",
    "plt.title('Feature Correlation Matrix (Before Selection)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================\n",
    "# Display Initial Mutual Information Scores\n",
    "# ============================\n",
    "\n",
    "# Calculate mutual information\n",
    "mi_scores = mutual_info_classif(X, y, discrete_features='auto', random_state=42)\n",
    "mi_scores = pd.Series(mi_scores, index=X.columns, name='MI Scores')\n",
    "mi_scores = mi_scores.sort_values(ascending=False)\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.barplot(x=mi_scores.values, y=mi_scores.index)\n",
    "plt.title('Mutual Information Scores (Before Selection)')\n",
    "plt.xlabel('Mutual Information')\n",
    "plt.ylabel('Features')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "# ============================\n",
    "# Correlation Filtering\n",
    "# ============================\n",
    "\n",
    "# Calculate the correlation matrix\n",
    "correlation_matrix = X.corr()\n",
    "correlation_threshold = 0.9\n",
    "correlated_features = set()\n",
    "\n",
    "# Identify features with correlation above the threshold\n",
    "for i in range(len(correlation_matrix.columns)):\n",
    "    for j in range(i):\n",
    "        if abs(correlation_matrix.iloc[i, j]) > correlation_threshold:\n",
    "            correlated_features.add(correlation_matrix.columns[i])\n",
    "\n",
    "# Keep only non-correlated features\n",
    "X = X.drop(columns=correlated_features)\n",
    "print(\"Remaining features correlation filtering:\", list(X))\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "# ============================\n",
    "# Mutual Information Filtering\n",
    "# ============================\n",
    "\n",
    "# Calculate mutual information\n",
    "mutual_info = mutual_info_classif(X, y, discrete_features='auto', random_state=42)\n",
    "mutual_info_series = pd.Series(mutual_info, index=X.columns)\n",
    "\n",
    "# Set a threshold for mutual information\n",
    "mi_threshold = 0.02\n",
    "selected_features_mi = mutual_info_series[mutual_info_series > mi_threshold].index.tolist()\n",
    "\n",
    "# Filter the dataset based on selected features\n",
    "X = X[selected_features_mi]\n",
    "print(\"Remaining features after mutual information filtering:\", selected_features_mi)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================\n",
    "# Define the XGBoost model\n",
    "# ============================\n",
    "\n",
    "def create_model(trial):\n",
    "    return XGBClassifier(\n",
    "        random_state=42,\n",
    "        eval_metric='logloss',\n",
    "        learning_rate=trial.suggest_float('learning_rate', 0.01, 0.3),\n",
    "        n_estimators=trial.suggest_int('n_estimators', 50, 500),\n",
    "        max_depth=trial.suggest_int('max_depth', 3, 10),\n",
    "        min_child_weight=trial.suggest_int('min_child_weight', 1, 10),\n",
    "        subsample=trial.suggest_float('subsample', 0.5, 1.0),\n",
    "        colsample_bytree=trial.suggest_float('colsample_bytree', 0.5, 1.0),\n",
    "        gamma=trial.suggest_float('gamma', 0, 5),\n",
    "        reg_alpha=trial.suggest_float('reg_alpha', 0, 1),\n",
    "        reg_lambda=trial.suggest_float('reg_lambda', 0, 1),\n",
    "        scale_pos_weight=trial.suggest_float('scale_pos_weight', 1, 100)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use StratifiedKFold to maintain class distribution in each fold\n",
    "skf = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)\n",
    "\n",
    "# Define specificity scorer\n",
    "def specificity(y_true, y_pred):\n",
    "    tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()\n",
    "    return tn / (tn + fp)\n",
    "\n",
    "specificity_scorer = make_scorer(specificity)\n",
    "\n",
    "# Define the scoring metrics\n",
    "scoring = {\n",
    "    'accuracy': 'accuracy',\n",
    "    'precision': 'precision',\n",
    "    'recall': 'recall',\n",
    "    'f1': 'f1',\n",
    "    'roc_auc': 'roc_auc',\n",
    "    'specificity': specificity_scorer\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "## ============================\n",
    "## Hyperparameter Tuning with Optuna\n",
    "## ============================\n",
    "\n",
    "#def objective(trial):\n",
    "#    model = create_model(trial)\n",
    "#    cv_results = cross_validate(model, X, y, cv=skf, scoring=['accuracy', 'roc_auc'], n_jobs=-1)\n",
    "#    mean_accuracy = np.mean(cv_results['test_accuracy'])\n",
    "#    mean_auc = np.mean(cv_results['test_roc_auc'])\n",
    "#\n",
    "#    return 0.5 * mean_accuracy + 0.5 * mean_auc\n",
    "\n",
    "#sampler = optuna.samplers.TPESampler(seed=42)\n",
    "#study = optuna.create_study(direction='maximize', sampler=sampler)\n",
    "#study.optimize(objective, n_trials=1000, n_jobs=-1)\n",
    "\n",
    "## Use the best hyperparameters to create the final model\n",
    "#best_params = study.best_params\n",
    "#print(\"Best Hyperparameters:\", best_params)\n",
    "#print(\"Best Value:\", study.best_value)\n",
    "\n",
    "#final_model = XGBClassifier(**best_params, random_state=42, eval_metric='logloss')\n",
    "\n",
    "final_model = XGBClassifier(\n",
    "    learning_rate=0.1,       # Step size shrinkage\n",
    "    n_estimators=100,        # Number of trees\n",
    "    max_depth=6,             # Maximum depth of trees\n",
    "    min_child_weight=1,      # Minimum sum of instance weight (hessian) in a child\n",
    "    subsample=0.8,           # Subsample ratio of training data\n",
    "    colsample_bytree=0.8,    # Subsample ratio of features per tree\n",
    "    gamma=0,                 # Minimum loss reduction to make a split\n",
    "    reg_alpha=0,             # L1 regularization\n",
    "    reg_lambda=1,            # L2 regularization\n",
    "    scale_pos_weight=1,      # Used for imbalanced datasets; adjust if needed\n",
    "    random_state=42,         # Ensures reproducibility\n",
    "    verbosity=1              # Controls the output verbosity\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================\n",
    "# Final Evaluation\n",
    "# ============================\n",
    "cv_results = cross_validate(\n",
    "    final_model, X, y, cv=skf, scoring=scoring, return_estimator=True, n_jobs=-1\n",
    ")\n",
    "\n",
    "feature_importances = []\n",
    "for estimator in cv_results['estimator']:\n",
    "    feature_importances.append(estimator.feature_importances_)\n",
    "\n",
    "average_importances = np.mean(feature_importances, axis=0)\n",
    "\n",
    "feature_names = X.columns\n",
    "importance_df = pd.DataFrame({\n",
    "    'Feature': feature_names,\n",
    "    'Average Importance': average_importances\n",
    "}).sort_values(by='Average Importance', ascending=False)\n",
    "\n",
    "# Display the feature importance table\n",
    "display(importance_df)\n",
    "\n",
    "# Plot the feature importances\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.barh(importance_df['Feature'], importance_df['Average Importance'])\n",
    "plt.xlabel('Average Importance')\n",
    "plt.ylabel('Feature')\n",
    "plt.title('Average Feature Importance Across Folds')\n",
    "plt.gca().invert_yaxis()\n",
    "plt.show()\n",
    "\n",
    "print(\"Mean Cross-Validation Accuracy:\", cv_results['test_accuracy'].mean())\n",
    "print(\"Mean Cross-Validation Precision:\", cv_results['test_precision'].mean())\n",
    "print(\"Mean Cross-Validation Recall:\", cv_results['test_recall'].mean())\n",
    "print(\"Mean Cross-Validation F1 Score:\", cv_results['test_f1'].mean())\n",
    "print(\"Mean Cross-Validation AUC Score:\", cv_results['test_roc_auc'].mean())\n",
    "print(\"Mean Cross-Validation Specificity:\", cv_results['test_specificity'].mean())\n",
    "\n",
    "# ROC Curve Visualization with Individual Folds\n",
    "plt.figure(figsize=(10, 8))\n",
    "mean_fpr = np.linspace(0, 1, 100)\n",
    "tprs = []\n",
    "aucs = []\n",
    "for i, (train_index, test_index) in enumerate(skf.split(X, y)):\n",
    "    X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "    y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "    \n",
    "    final_model.fit(X_train, y_train)\n",
    "    y_proba = final_model.predict_proba(X_test)[:, 1]\n",
    "    \n",
    "    fpr, tpr, _ = roc_curve(y_test, y_proba)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    aucs.append(roc_auc)\n",
    "    tprs.append(np.interp(mean_fpr, fpr, tpr))\n",
    "    tprs[-1][0] = 0.0\n",
    "    \n",
    "    plt.plot(fpr, tpr, lw=1, alpha=0.3, label=f'Fold {i+1} (AUC = {roc_auc:.2f})')\n",
    "\n",
    "# Plot the mean ROC curve\n",
    "mean_tpr = np.mean(tprs, axis=0)\n",
    "mean_tpr[-1] = 1.0\n",
    "mean_auc = auc(mean_fpr, mean_tpr)\n",
    "plt.plot(mean_fpr, mean_tpr, color='b', label=f'Mean ROC (AUC = {mean_auc:.2f})', lw=2)\n",
    "\n",
    "# Plot the random chance line\n",
    "plt.plot([0, 1], [0, 1], linestyle='--', color='r', label='Random Chance')\n",
    "\n",
    "plt.xlabel('False Positive Rate (1 - Specificity)')\n",
    "plt.ylabel('True Positive Rate (Sensitivity)')\n",
    "plt.title('ROC Curves with Individual Folds and Mean')\n",
    "plt.legend(loc='lower right')\n",
    "plt.grid(alpha=0.3)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
