{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 427,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from imblearn.over_sampling import SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 428,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_missing_values(df):\n",
    "    missing_data = df.isnull().sum()\n",
    "    missing_data = missing_data[missing_data > 0]\n",
    "    return missing_data.sort_values(ascending=False)\n",
    "\n",
    "def detect_outliers(df, columns):\n",
    "    outliers = {}\n",
    "    for column in columns:\n",
    "            # Use IQR method\n",
    "            Q1 = df[column].quantile(0.25)\n",
    "            Q3 = df[column].quantile(0.75)\n",
    "            IQR = Q3 - Q1\n",
    "            lower_bound = Q1 - 1.5 * IQR\n",
    "            upper_bound = Q3 + 1.5 * IQR\n",
    "            outlier_rows = df[(df[column] < lower_bound) | (df[column] > upper_bound)]\n",
    "            outliers[column] = outlier_rows[[column]]\n",
    "    \n",
    "    return outliers\n",
    "\n",
    "def detect_duplicates(df):\n",
    "    duplicate_rows = df[df.duplicated()]\n",
    "    return duplicate_rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 429,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dropped redundant rebound columns and renamed others for clarity.\n",
      "Dropped 'divID' column as it contains no information.\n",
      "Dropped 'seeded' column as it contains only zero values.\n",
      "Dropped 'lgID', 'franchID', 'confID', 'name', and 'arena' as they are irrelevant for predictive modeling.\n",
      "\n",
      "Missing Values\n",
      "Series([], dtype: int64)\n",
      "\n",
      "Duplicates\n",
      "Empty DataFrame\n",
      "Columns: [year, tmID, rank, playoff, o_fgm, o_fga, o_ftm, o_fta, o_3pm, o_3pa, o_oreb, o_dreb, o_reb, o_asts, o_pf, o_stl, o_to, o_blk, o_pts, d_fgm, d_fga, d_ftm, d_fta, d_3pm, d_3pa, d_oreb, d_dreb, d_reb, d_asts, d_pf, d_stl, d_to, d_blk, d_pts, won, lost, GP, homeW, homeL, awayW, awayL, confW, confL, min, attend, playoff_progression_score]\n",
      "Index: []\n",
      "\n",
      "[0 rows x 46 columns]\n"
     ]
    }
   ],
   "source": [
    "# Load the teams_post data\n",
    "teams = pd.read_csv('./data/teams.csv')\n",
    "\n",
    "# Convert 'playoff' column to binary (1 for 'Y', 0 for 'N')\n",
    "teams['playoff'] = teams['playoff'].map({'Y': 1, 'N': 0})\n",
    "\n",
    "zero_cols = [\"tmORB\", \"tmDRB\", \"tmTRB\", \"opptmORB\", \"opptmDRB\", \"opptmTRB\"]\n",
    "\n",
    "# Since \"tmORB\", \"tmDRB\", \"tmTRB\", \"opptmORB\", \"opptmDRB\", and \"opptmTRB\" contain only zero values,\n",
    "# and are redundant with \"o_oreb\", \"o_dreb\", \"o_reb\" for team stats, and \"d_oreb\", \"d_dreb\", \"d_reb\" for opponent stats,\n",
    "# we drop the redundant columns.\n",
    "\n",
    "teams = teams.drop(columns=zero_cols)\n",
    "print(\"Dropped redundant rebound columns and renamed others for clarity.\")\n",
    "\n",
    "# Drop the 'divID' column as it contains only empty strings and does not add useful information\n",
    "teams = teams.drop(columns=['divID'])\n",
    "print(\"Dropped 'divID' column as it contains no information.\")\n",
    "\n",
    "# Drop the 'seeded' column as it contains only zero values\n",
    "teams = teams.drop(columns=['seeded'])\n",
    "print(\"Dropped 'seeded' column as it contains only zero values.\")\n",
    "\n",
    "def calculate_playoff_score(row):\n",
    "    if row['finals'] == 'W':\n",
    "        return 4  # Won the championship\n",
    "    elif row['finals'] == 'L':\n",
    "        return 3  # Lost in the finals\n",
    "    elif row['semis'] == 'L':\n",
    "        return 2  # Lost in the semifinals\n",
    "    elif row['firstRound'] == 'L':\n",
    "        return 1  # Lost in the first round\n",
    "    else:\n",
    "        return 0  # Did not make the playoffs\n",
    "\n",
    "# Apply the function to each row to create the playoff_progression_score\n",
    "teams['playoff_progression_score'] = teams.apply(calculate_playoff_score, axis=1)\n",
    "\n",
    "# Drop the original 'firstRound', 'semis', and 'finals' columns as they are now redundant\n",
    "teams = teams.drop(columns=['firstRound', 'semis', 'finals'])\n",
    "\n",
    "# Drop columns that don't add predictive value\n",
    "# 'lgID': Contains only \"WNBA\" for every row, so it provides no additional information.\n",
    "# 'franchID': Redundant identifier, as 'tmID' already identifies each team uniquely.\n",
    "# 'confID': Lacks value without conference-specific qualification/matchup data.\n",
    "# 'name': Purely descriptive and irrelevant to playoff predictions.\n",
    "# 'arena': Also descriptive and does not impact playoff qualification.\n",
    "teams = teams.drop(columns=['lgID', 'franchID', 'confID', 'name', 'arena'])\n",
    "print(\"Dropped 'lgID', 'franchID', 'confID', 'name', and 'arena' as they are irrelevant for predictive modeling.\")\n",
    "\n",
    "# DETECTION OF MISSING VALUES\n",
    "missing_values = detect_missing_values(teams)\n",
    "print(\"\\nMissing Values\")\n",
    "print(missing_values)\n",
    "\n",
    "# DETECTION OF DUPLICATES\n",
    "duplicate_rows = detect_duplicates(teams)\n",
    "print(\"\\nDuplicates\")\n",
    "print(duplicate_rows)\n",
    "\n",
    "# DETECTION OF OUTLIERS\n",
    "# Select numerical columns only\n",
    "numeric_columns = teams.select_dtypes(include=['float64', 'int64']).columns\n",
    "\n",
    "#for column in numeric_columns:\n",
    "#    plt.figure(figsize=(8, 4))\n",
    "#    sns.boxplot(x=teams[column])\n",
    "#    plt.title(f'Box Plot of {column}')\n",
    "#    plt.show()\n",
    "#\n",
    "#outliers = detect_outliers(teams, numeric_columns)\n",
    "#\n",
    "#for col, outlier_data in outliers.items():\n",
    "#    print(f\"Outliers in {col}:\\n{outlier_data}\\n\")\n",
    "\n",
    "## Plot for 'next_season_playoff' column\n",
    "#plt.figure(figsize=(6, 4))\n",
    "#plt.bar(next_playoff_counts.index, next_playoff_counts.values)\n",
    "#plt.title('Data Balance in Next Season Playoff')\n",
    "#plt.xlabel('Next Season Playoff (0 = No, 1 = Yes)')\n",
    "#plt.ylabel('Count')\n",
    "#plt.xticks([0, 1])\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 430,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Correlation between offensive statistics\n",
    "## -------------------------\n",
    "#\n",
    "#o_stats = teams.filter(regex='^(o_)')\n",
    "#\n",
    "#corr_matrix = o_stats.corr()\n",
    "#\n",
    "#plt.figure(figsize=(14, 12))\n",
    "#\n",
    "#cmap = sns.color_palette(\"mako\", as_cmap=True)\n",
    "#\n",
    "#sns.heatmap(corr_matrix, cmap=cmap, vmax=1.0, vmin=-1.0, center=0,\n",
    "#            square=True, linewidths=.5, annot=True, fmt=\".2f\", annot_kws={\"size\":8})\n",
    "#\n",
    "#plt.title('Correlation Heatmap of Performance Statistics', fontsize=18)\n",
    "#plt.xticks(rotation=45, ha='right', fontsize=10)\n",
    "#plt.yticks(fontsize=10)\n",
    "#plt.tight_layout()\n",
    "#\n",
    "## Display the heatmap\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 431,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Correlation between defensive statistics\n",
    "## -------------------------\n",
    "#\n",
    "#d_stats = teams.filter(regex='^(d_)')\n",
    "#\n",
    "#corr_matrix = d_stats.corr()\n",
    "#\n",
    "#plt.figure(figsize=(14, 12))\n",
    "#\n",
    "#cmap = sns.color_palette(\"coolwarm\", as_cmap=True)\n",
    "#\n",
    "#sns.heatmap(corr_matrix, cmap=cmap, vmax=1.0, vmin=-1.0, center=0,\n",
    "#            square=True, linewidths=.5, annot=True, fmt=\".2f\", annot_kws={\"size\":8})\n",
    "#\n",
    "#plt.title('Correlation Heatmap of Performance Statistics', fontsize=18)\n",
    "#plt.xticks(rotation=45, ha='right', fontsize=10)\n",
    "#plt.yticks(fontsize=10)\n",
    "#plt.tight_layout()\n",
    "#\n",
    "## Display the heatmap\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 432,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Compute correlation between offensive and defensive stats\n",
    "#o_stats = teams.filter(regex='^(o_)')\n",
    "#d_stats = teams.filter(regex='^(d_)')\n",
    "#\n",
    "#combined_stats = pd.concat([o_stats, d_stats], axis=1)\n",
    "#\n",
    "#corr_matrix = combined_stats.corr()\n",
    "#\n",
    "#o_d_corr_matrix = corr_matrix.loc[o_stats.columns, d_stats.columns]\n",
    "#\n",
    "#plt.figure(figsize=(12, 10))\n",
    "#sns.heatmap(o_d_corr_matrix, annot=True, fmt=\".2f\", cmap=\"coolwarm\", center=0,\n",
    "#            linewidths=.5, square=True, cbar_kws={\"shrink\": .75})\n",
    "#\n",
    "#plt.title(\"Correlation Between Offensive and Defensive Statistics\")\n",
    "#plt.xticks(rotation=45, ha='right', fontsize=10)\n",
    "#plt.yticks(fontsize=10)\n",
    "#plt.tight_layout()\n",
    "#\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 433,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate statistics for the new dataframe\n",
    "dataset = pd.DataFrame()\n",
    "\n",
    "# Adding team statistics\n",
    "#dataset['Playoff'] = teams['playoff']\n",
    "dataset['Rank'] = teams['rank']\n",
    "dataset['PlayoffProgScore'] = teams['playoff_progression_score']\n",
    "dataset['GP'] = teams['GP']\n",
    "dataset['W'] = teams['won']\n",
    "dataset['L'] = teams['lost']\n",
    "dataset['WIN%'] = 100 * (teams['won'] / teams['GP'])\n",
    "dataset['MIN'] = teams['min']\n",
    "dataset['PTS'] = teams['o_pts']\n",
    "dataset['FGM'] = teams['o_fgm']\n",
    "dataset['FGA'] = teams['o_fga']\n",
    "dataset['FG%'] = 100 * (teams['o_fgm'] / teams['o_fga'])\n",
    "dataset['3PM'] = teams['o_3pm']\n",
    "dataset['3PA'] = teams['o_3pa']\n",
    "dataset['3P%'] = 100 * (teams['o_3pm'] / teams['o_3pa'])\n",
    "dataset['FTM'] = teams['o_ftm']\n",
    "dataset['FTA'] = teams['o_fta']\n",
    "dataset['FT%'] = 100 * (teams['o_ftm'] / teams['o_fta'])\n",
    "dataset['OREB'] = teams['o_oreb']\n",
    "dataset['DREB'] = teams['o_dreb']\n",
    "dataset['REB'] = teams['o_reb']\n",
    "dataset['AST'] = teams['o_asts']\n",
    "dataset['TOV'] = teams['o_to']\n",
    "dataset['STL'] = teams['o_stl']\n",
    "dataset['BLK'] = teams['o_blk']\n",
    "dataset['BLKA'] = teams['d_blk']\n",
    "dataset['PF'] = teams['o_pf']\n",
    "dataset['PFD'] = teams['d_pf']\n",
    "\n",
    "# Advanced\n",
    "dataset['POSS'] = 0.5 * (\n",
    "    (teams['o_fga'] + 0.4 * teams['o_fta'] -\n",
    "     1.07 * (teams['o_oreb'] / (teams['o_oreb'] + teams['d_dreb'])) *\n",
    "     (teams['o_fga'] - teams['o_fgm']) + teams['o_to']) +\n",
    "    (teams['d_fga'] + 0.4 * teams['d_fta'] -\n",
    "     1.07 * (teams['d_oreb'] / (teams['d_oreb'] + teams['o_dreb'])) *\n",
    "     (teams['d_fga'] - teams['d_fgm']) + teams['d_to'])\n",
    ")\n",
    "dataset['OFFRTG'] = 100 * (teams['o_pts'] / dataset['POSS'])\n",
    "dataset['DEFRTG'] = 100 * (teams['d_pts'] / dataset['POSS'])\n",
    "dataset['NETRTG'] = dataset['OFFRTG'] - dataset['DEFRTG']\n",
    "dataset['AST/TO'] = teams['o_asts'] / teams['o_to']\n",
    "dataset['AST RATIO'] = (teams['o_asts'] * 100) / dataset['POSS']\n",
    "dataset['OREB%'] = (\n",
    "    100 * (teams['o_oreb'] * (dataset['MIN'] / 5)) / \n",
    "    (dataset['MIN'] * (teams['o_oreb'] + teams['d_dreb']))\n",
    ")\n",
    "dataset['DREB%'] = (\n",
    "    100 * (teams['o_dreb'] * (dataset['MIN'] / 5)) / \n",
    "    (dataset['MIN'] * (teams['o_dreb'] + teams['d_oreb']))\n",
    ")\n",
    "dataset['REB%'] = (\n",
    "    100 * (teams['o_reb'] * (dataset['MIN'] / 5)) / \n",
    "    (dataset['MIN'] * (teams['o_reb'] + teams['d_reb']))\n",
    ")\n",
    "dataset['TOV%'] = 100 * teams['o_to'] / (\n",
    "    teams['o_fga'] + 0.44 * teams['o_fta'] + teams['o_to']\n",
    ")\n",
    "dataset['EFG%'] = 100 * ((teams['o_fgm'] + (0.5 * teams['o_3pm'])) / teams['o_fga'])\n",
    "dataset['TS%'] = 100 * (teams['o_pts'] / (2 * (teams['o_fga'] + 0.44 * teams['o_fta'])))\n",
    "\n",
    "OPPPOSS = 0.5 * (\n",
    "    (teams['d_fga'] + 0.4 * teams['d_fta'] -\n",
    "     1.07 * (teams['d_oreb'] / (teams['d_oreb'] + teams['o_dreb'])) *\n",
    "     (teams['d_fga'] - teams['d_fgm']) + teams['d_to']) +\n",
    "    (teams['o_fga'] + 0.4 * teams['o_fta'] -\n",
    "     1.07 * (teams['o_oreb'] / (teams['o_oreb'] + teams['d_dreb'])) *\n",
    "     (teams['o_fga'] - teams['o_fgm']) + teams['o_to'])\n",
    ")\n",
    "dataset['PACE'] = 40 * ((dataset['POSS'] + OPPPOSS) / (2 * (dataset['MIN'] / 5)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 434,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Label\n",
    "teams = teams.sort_values(by=['tmID', 'year']).reset_index(drop=True)\n",
    "dataset['PlayoffNextSeason'] = teams.groupby('tmID')['playoff'].shift(-1)\n",
    "dataset = dataset.dropna(subset=['PlayoffNextSeason'])\n",
    "dataset['PlayoffNextSeason'] = dataset['PlayoffNextSeason'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 435,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PlayoffNextSeason\n",
       "1    71\n",
       "0    51\n",
       "Name: count, dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "PlayoffNextSeason\n",
       "1    71\n",
       "0    71\n",
       "Name: count, dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Separate features and target variable\n",
    "X = dataset.drop(columns=['PlayoffNextSeason'])\n",
    "y = dataset['PlayoffNextSeason']\n",
    "\n",
    "display(y.value_counts())\n",
    "\n",
    "# Apply SMOTE to balance the classes\n",
    "smote = SMOTE(random_state=42)\n",
    "X_resampled, y_resampled = smote.fit_resample(X, y)\n",
    "\n",
    "# Verify the new class distribution after applying SMOTE\n",
    "display(y_resampled.value_counts())\n",
    "\n",
    "# Merge resampled data into a new DataFrame\n",
    "balanced_dataset = pd.DataFrame(X_resampled, columns=X.columns)\n",
    "balanced_dataset['PlayoffNextSeason'] = y_resampled\n",
    "\n",
    "dataset = balanced_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 436,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed dataset saved to ./cleaned_data/dataset.csv\n"
     ]
    }
   ],
   "source": [
    "# Save the new dataset to a CSV file\n",
    "output_file_path = './cleaned_data/dataset.csv'\n",
    "dataset.to_csv(output_file_path, index=False)\n",
    "\n",
    "print(f\"Processed dataset saved to {output_file_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cells bellow are to be used when cleaning, transforming and analyzing the remaining datasets. For now we are only studying teams.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 437,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n# Load the teams_post data\\nteams_post = pd.read_csv(\"./data/teams_post.csv\")\\n\\n# Drop \\'lgID\\' column as it contains only \"WNBA\" for every row\\nteams_post = teams_post.drop(columns=[\\'lgID\\'])\\n\\n# Detect and drop duplicates\\nduplicates = teams_post[teams_post.duplicated()]\\nif not duplicates.empty:\\n    print(\"Duplicates detected. Removing duplicate rows.\")\\n    teams_post = teams_post.drop_duplicates()\\nelse:\\n    print(\"No duplicates found.\")\\n\\n# Display a sample of the dataframe to verify changes\\ndisplay(teams_post.head())\\n\\n# Store cleaned csv\\nteams_post.to_csv(\\'./cleaned_data/teams_post.csv\\', index=False)\\n'"
      ]
     },
     "execution_count": 437,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "# Load the teams_post data\n",
    "teams_post = pd.read_csv(\"./data/teams_post.csv\")\n",
    "\n",
    "# Drop 'lgID' column as it contains only \"WNBA\" for every row\n",
    "teams_post = teams_post.drop(columns=['lgID'])\n",
    "\n",
    "# Detect and drop duplicates\n",
    "duplicates = teams_post[teams_post.duplicated()]\n",
    "if not duplicates.empty:\n",
    "    print(\"Duplicates detected. Removing duplicate rows.\")\n",
    "    teams_post = teams_post.drop_duplicates()\n",
    "else:\n",
    "    print(\"No duplicates found.\")\n",
    "\n",
    "# Display a sample of the dataframe to verify changes\n",
    "display(teams_post.head())\n",
    "\n",
    "# Store cleaned csv\n",
    "teams_post.to_csv('./cleaned_data/teams_post.csv', index=False)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 438,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n# Load the series_post data\\nseries_post = pd.read_csv(\"./data/series_post.csv\")\\n\\n# Drop \\'lgIDWinner\\' and \\'lgIDLoser\\' columns as they contain only \"WNBA\" and add no value\\nseries_post = series_post.drop(columns=[\\'lgIDWinner\\', \\'lgIDLoser\\'])\\n\\n# Detect and drop duplicates\\nduplicates = series_post[series_post.duplicated()]\\nif not duplicates.empty:\\n    print(\"Duplicates detected. Removing duplicate rows.\")\\n    series_post = series_post.drop_duplicates()\\nelse:\\n    print(\"No duplicates found.\")\\n\\n# Display a sample of the dataframe to verify changes\\ndisplay(series_post.head())\\n\\n# Store cleaned csv\\nseries_post.to_csv(\\'./cleaned_data/series_post.csv\\', index=False)\\n'"
      ]
     },
     "execution_count": 438,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "# Load the series_post data\n",
    "series_post = pd.read_csv(\"./data/series_post.csv\")\n",
    "\n",
    "# Drop 'lgIDWinner' and 'lgIDLoser' columns as they contain only \"WNBA\" and add no value\n",
    "series_post = series_post.drop(columns=['lgIDWinner', 'lgIDLoser'])\n",
    "\n",
    "# Detect and drop duplicates\n",
    "duplicates = series_post[series_post.duplicated()]\n",
    "if not duplicates.empty:\n",
    "    print(\"Duplicates detected. Removing duplicate rows.\")\n",
    "    series_post = series_post.drop_duplicates()\n",
    "else:\n",
    "    print(\"No duplicates found.\")\n",
    "\n",
    "# Display a sample of the dataframe to verify changes\n",
    "display(series_post.head())\n",
    "\n",
    "# Store cleaned csv\n",
    "series_post.to_csv('./cleaned_data/series_post.csv', index=False)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 439,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n# Load the coaches data\\ncoaches = pd.read_csv(\"./data/coaches.csv\")\\n\\n# Drop \\'lgID\\' as it is only \"WNBA\" and provides no unique value\\ncoaches = coaches.drop(columns=[\\'lgID\\'])\\n\\n# Detect and drop duplicates\\nduplicates = coaches[coaches.duplicated()]\\nif not duplicates.empty:\\n    print(\"Duplicates detected. Removing duplicate rows.\")\\n    coaches = coaches.drop_duplicates()\\nelse:\\n    print(\"No duplicates found.\")\\n\\n# Display a sample of the dataframe to verify changes\\ndisplay(coaches.head())\\n\\n# Store cleaned csv\\ncoaches.to_csv(\\'./cleaned_data/coaches.csv\\', index=False)\\n'"
      ]
     },
     "execution_count": 439,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "# Load the coaches data\n",
    "coaches = pd.read_csv(\"./data/coaches.csv\")\n",
    "\n",
    "# Drop 'lgID' as it is only \"WNBA\" and provides no unique value\n",
    "coaches = coaches.drop(columns=['lgID'])\n",
    "\n",
    "# Detect and drop duplicates\n",
    "duplicates = coaches[coaches.duplicated()]\n",
    "if not duplicates.empty:\n",
    "    print(\"Duplicates detected. Removing duplicate rows.\")\n",
    "    coaches = coaches.drop_duplicates()\n",
    "else:\n",
    "    print(\"No duplicates found.\")\n",
    "\n",
    "# Display a sample of the dataframe to verify changes\n",
    "display(coaches.head())\n",
    "\n",
    "# Store cleaned csv\n",
    "coaches.to_csv('./cleaned_data/coaches.csv', index=False)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 440,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n# Load the players_teams data\\nplayers_teams = pd.read_csv(\"./data/players_teams.csv\")\\n\\n# Drop \\'lgID\\' as it contains only \"WNBA\" and provides no unique value\\nplayers_teams = players_teams.drop(columns=[\\'lgID\\'])\\n\\n# Detect and drop duplicates\\nduplicates = players_teams[players_teams.duplicated()]\\nif not duplicates.empty:\\n    print(\"Duplicates detected. Removing duplicate rows.\")\\n    players_teams = players_teams.drop_duplicates()\\nelse:\\n    print(\"No duplicates found.\")\\n\\n# Display a sample of the dataframe to verify changes\\ndisplay(players_teams.head())\\n\\n# Store cleaned csv\\nplayers_teams.to_csv(\\'./cleaned_data/players_teams.csv\\', index=False)\\n'"
      ]
     },
     "execution_count": 440,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "# Load the players_teams data\n",
    "players_teams = pd.read_csv(\"./data/players_teams.csv\")\n",
    "\n",
    "# Drop 'lgID' as it contains only \"WNBA\" and provides no unique value\n",
    "players_teams = players_teams.drop(columns=['lgID'])\n",
    "\n",
    "# Detect and drop duplicates\n",
    "duplicates = players_teams[players_teams.duplicated()]\n",
    "if not duplicates.empty:\n",
    "    print(\"Duplicates detected. Removing duplicate rows.\")\n",
    "    players_teams = players_teams.drop_duplicates()\n",
    "else:\n",
    "    print(\"No duplicates found.\")\n",
    "\n",
    "# Display a sample of the dataframe to verify changes\n",
    "display(players_teams.head())\n",
    "\n",
    "# Store cleaned csv\n",
    "players_teams.to_csv('./cleaned_data/players_teams.csv', index=False)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 441,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n# Load the players data\\nplayers = pd.read_csv(\"./data/players.csv\")\\n\\n# Filter out rows in players that do not have corresponding playerIDs in players_teams\\nvalid_player_ids = players_teams[\\'playerID\\'].unique()\\nplayers = players[players[\\'bioID\\'].isin(valid_player_ids)]\\n\\n# Show that all values in firstseason and lastseason are \\'0\\'\\nfirstseason_all_zero = (players[\\'firstseason\\'] == 0).all()\\nlastseason_all_zero = (players[\\'lastseason\\'] == 0).all()\\n\\nprint(\"All values in \\'firstseason\\' are 0:\", firstseason_all_zero)\\nprint(\"All values in \\'lastseason\\' are 0:\", lastseason_all_zero)\\n\\n# Show the only valida player with a registered Death Date\\nnon_zero_death_dates = players[players[\\'deathDate\\'] != \"0000-00-00\"]\\ndisplay(non_zero_death_dates.head())\\n\\n# Even though there is 1 registered Death Date, it really doesn\\'t add anything. Birth Date is kept, for potential aging information.\\nplayers = players.drop(columns=[\\'firstseason\\', \\'lastseason\\', \\'deathDate\\'])\\nprint(\"Dropped \\'firstseason\\', \\'lastseason\\', and \\'deathDate\\' columns as they contain only irrelevant values.\")\\n\\n# Detect and drop duplicates\\nduplicates = players[players.duplicated()]\\nif not duplicates.empty:\\n    print(\"Duplicates detected. Removing duplicate rows.\")\\n    players = players.drop_duplicates()\\nelse:\\n    print(\"No duplicates found.\")\\n\\n# Display a sample of the dataframe to verify changes\\ndisplay(players.head())\\n\\n# Store cleaned csv\\nplayers.to_csv(\\'./cleaned_data/players.csv\\', index=False)\\n'"
      ]
     },
     "execution_count": 441,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "# Load the players data\n",
    "players = pd.read_csv(\"./data/players.csv\")\n",
    "\n",
    "# Filter out rows in players that do not have corresponding playerIDs in players_teams\n",
    "valid_player_ids = players_teams['playerID'].unique()\n",
    "players = players[players['bioID'].isin(valid_player_ids)]\n",
    "\n",
    "# Show that all values in firstseason and lastseason are '0'\n",
    "firstseason_all_zero = (players['firstseason'] == 0).all()\n",
    "lastseason_all_zero = (players['lastseason'] == 0).all()\n",
    "\n",
    "print(\"All values in 'firstseason' are 0:\", firstseason_all_zero)\n",
    "print(\"All values in 'lastseason' are 0:\", lastseason_all_zero)\n",
    "\n",
    "# Show the only valida player with a registered Death Date\n",
    "non_zero_death_dates = players[players['deathDate'] != \"0000-00-00\"]\n",
    "display(non_zero_death_dates.head())\n",
    "\n",
    "# Even though there is 1 registered Death Date, it really doesn't add anything. Birth Date is kept, for potential aging information.\n",
    "players = players.drop(columns=['firstseason', 'lastseason', 'deathDate'])\n",
    "print(\"Dropped 'firstseason', 'lastseason', and 'deathDate' columns as they contain only irrelevant values.\")\n",
    "\n",
    "# Detect and drop duplicates\n",
    "duplicates = players[players.duplicated()]\n",
    "if not duplicates.empty:\n",
    "    print(\"Duplicates detected. Removing duplicate rows.\")\n",
    "    players = players.drop_duplicates()\n",
    "else:\n",
    "    print(\"No duplicates found.\")\n",
    "\n",
    "# Display a sample of the dataframe to verify changes\n",
    "display(players.head())\n",
    "\n",
    "# Store cleaned csv\n",
    "players.to_csv('./cleaned_data/players.csv', index=False)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 442,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n# Load the awards_players data\\nawards_players = pd.read_csv(\"./data/awards_players.csv\")\\n\\n# Drop \\'lgID\\' column as it provides no unique value\\nawards_players = awards_players.drop(columns=[\\'lgID\\'])\\n\\n# Separate dataframes for player awards and coach awards\\nplayer_awards = awards_players[~awards_players[\\'award\\'].str.contains(\"Coach\")].copy()\\ncoach_awards = awards_players[awards_players[\\'award\\'].str.contains(\"Coach\")].copy()\\n\\n# Standardize award names\\naward_name_mapping = {\\n    \"Kim Perrot Sportsmanship\": \"Kim Perrot Sportsmanship Award\",\\n    \"Kim Perrot Sportsmanship Award\": \"Kim Perrot Sportsmanship Award\",\\n    \"All-Star Game Most Valuable Player\": \"All-Star Game MVP\",\\n    \"Most Valuable Player\": \"MVP\",\\n    \"WNBA Finals Most Valuable Player\": \"Finals MVP\",\\n    \"Sixth Woman of the Year\": \"6th Woman of the Year\",\\n    \"WNBA All-Decade Team\": \"All-Decade Team\",\\n    \"WNBA All Decade Team Honorable Mention\": \"All-Decade Team Honorable Mention\"\\n}\\n\\nplayer_awards.loc[:, \\'award\\'] = player_awards[\\'award\\'].map(award_name_mapping).fillna(player_awards[\\'award\\'])\\ncoach_awards.loc[:, \\'award\\'] = coach_awards[\\'award\\'].map(award_name_mapping).fillna(coach_awards[\\'award\\'])\\n\\n# Detect and drop duplicates\\nduplicates_player = player_awards[player_awards.duplicated()]\\nif not duplicates_player.empty:\\n    print(\"Duplicates detected in player awards. Removing duplicate rows.\")\\n    player_awards = player_awards.drop_duplicates()\\nelse:\\n    print(\"No duplicates found in player awards.\")\\n\\nduplicates_coach = coach_awards[coach_awards.duplicated()]\\nif not duplicates_coach.empty:\\n    print(\"Duplicates detected in coach awards. Removing duplicate rows.\")\\n    coach_awards = coach_awards.drop_duplicates()\\nelse:\\n    print(\"No duplicates found in coach awards.\")\\n\\n# Display samples of both dataframes to verify transformations\\ndisplay(player_awards.head())\\ndisplay(coach_awards.head())\\n\\n# Store cleaned csvs\\nplayer_awards.to_csv(\\'./cleaned_data/player_awards.csv\\', index=False)\\ncoach_awards.to_csv(\\'./cleaned_data/coach_awards.csv\\', index=False)\\n'"
      ]
     },
     "execution_count": 442,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "# Load the awards_players data\n",
    "awards_players = pd.read_csv(\"./data/awards_players.csv\")\n",
    "\n",
    "# Drop 'lgID' column as it provides no unique value\n",
    "awards_players = awards_players.drop(columns=['lgID'])\n",
    "\n",
    "# Separate dataframes for player awards and coach awards\n",
    "player_awards = awards_players[~awards_players['award'].str.contains(\"Coach\")].copy()\n",
    "coach_awards = awards_players[awards_players['award'].str.contains(\"Coach\")].copy()\n",
    "\n",
    "# Standardize award names\n",
    "award_name_mapping = {\n",
    "    \"Kim Perrot Sportsmanship\": \"Kim Perrot Sportsmanship Award\",\n",
    "    \"Kim Perrot Sportsmanship Award\": \"Kim Perrot Sportsmanship Award\",\n",
    "    \"All-Star Game Most Valuable Player\": \"All-Star Game MVP\",\n",
    "    \"Most Valuable Player\": \"MVP\",\n",
    "    \"WNBA Finals Most Valuable Player\": \"Finals MVP\",\n",
    "    \"Sixth Woman of the Year\": \"6th Woman of the Year\",\n",
    "    \"WNBA All-Decade Team\": \"All-Decade Team\",\n",
    "    \"WNBA All Decade Team Honorable Mention\": \"All-Decade Team Honorable Mention\"\n",
    "}\n",
    "\n",
    "player_awards.loc[:, 'award'] = player_awards['award'].map(award_name_mapping).fillna(player_awards['award'])\n",
    "coach_awards.loc[:, 'award'] = coach_awards['award'].map(award_name_mapping).fillna(coach_awards['award'])\n",
    "\n",
    "# Detect and drop duplicates\n",
    "duplicates_player = player_awards[player_awards.duplicated()]\n",
    "if not duplicates_player.empty:\n",
    "    print(\"Duplicates detected in player awards. Removing duplicate rows.\")\n",
    "    player_awards = player_awards.drop_duplicates()\n",
    "else:\n",
    "    print(\"No duplicates found in player awards.\")\n",
    "\n",
    "duplicates_coach = coach_awards[coach_awards.duplicated()]\n",
    "if not duplicates_coach.empty:\n",
    "    print(\"Duplicates detected in coach awards. Removing duplicate rows.\")\n",
    "    coach_awards = coach_awards.drop_duplicates()\n",
    "else:\n",
    "    print(\"No duplicates found in coach awards.\")\n",
    "\n",
    "# Display samples of both dataframes to verify transformations\n",
    "display(player_awards.head())\n",
    "display(coach_awards.head())\n",
    "\n",
    "# Store cleaned csvs\n",
    "player_awards.to_csv('./cleaned_data/player_awards.csv', index=False)\n",
    "coach_awards.to_csv('./cleaned_data/coach_awards.csv', index=False)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
