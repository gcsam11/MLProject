{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 594,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from imblearn.over_sampling import SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 595,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_missing_values(df):\n",
    "    missing_data = df.isnull().sum()\n",
    "    missing_data = missing_data[missing_data > 0]\n",
    "    return missing_data.sort_values(ascending=False)\n",
    "\n",
    "def detect_outliers(df, columns):\n",
    "    outliers = {}\n",
    "    for column in columns:\n",
    "            # Use IQR method\n",
    "            Q1 = df[column].quantile(0.25)\n",
    "            Q3 = df[column].quantile(0.75)\n",
    "            IQR = Q3 - Q1\n",
    "            lower_bound = Q1 - 1.5 * IQR\n",
    "            upper_bound = Q3 + 1.5 * IQR\n",
    "            outlier_rows = df[(df[column] < lower_bound) | (df[column] > upper_bound)]\n",
    "            outliers[column] = outlier_rows[[column]]\n",
    "    \n",
    "    return outliers\n",
    "\n",
    "def detect_duplicates(df):\n",
    "    duplicate_rows = df[df.duplicated()]\n",
    "    return duplicate_rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 596,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dropped redundant rebound columns and renamed others for clarity.\n",
      "Dropped 'divID' column as it contains no information.\n",
      "Dropped 'seeded' column as it contains only zero values.\n",
      "Dropped 'lgID', 'franchID', 'confID', 'name', and 'arena' as they are irrelevant for predictive modeling.\n",
      "\n",
      "Missing Values\n",
      "Series([], dtype: int64)\n",
      "\n",
      "Duplicates\n",
      "Empty DataFrame\n",
      "Columns: [year, tmID, rank, playoff, o_fgm, o_fga, o_ftm, o_fta, o_3pm, o_3pa, o_oreb, o_dreb, o_reb, o_asts, o_pf, o_stl, o_to, o_blk, o_pts, d_fgm, d_fga, d_ftm, d_fta, d_3pm, d_3pa, d_oreb, d_dreb, d_reb, d_asts, d_pf, d_stl, d_to, d_blk, d_pts, won, lost, GP, homeW, homeL, awayW, awayL, confW, confL, min, attend, playoff_progression_score]\n",
      "Index: []\n",
      "\n",
      "[0 rows x 46 columns]\n"
     ]
    }
   ],
   "source": [
    "# Load the teams_post data\n",
    "teams = pd.read_csv('./data/teams.csv')\n",
    "\n",
    "# Convert 'playoff' column to binary (1 for 'Y', 0 for 'N')\n",
    "teams['playoff'] = teams['playoff'].map({'Y': 1, 'N': 0})\n",
    "\n",
    "zero_cols = [\"tmORB\", \"tmDRB\", \"tmTRB\", \"opptmORB\", \"opptmDRB\", \"opptmTRB\"]\n",
    "\n",
    "# Since \"tmORB\", \"tmDRB\", \"tmTRB\", \"opptmORB\", \"opptmDRB\", and \"opptmTRB\" contain only zero values,\n",
    "# and are redundant with \"o_oreb\", \"o_dreb\", \"o_reb\" for team stats, and \"d_oreb\", \"d_dreb\", \"d_reb\" for opponent stats,\n",
    "# we drop the redundant columns.\n",
    "\n",
    "teams = teams.drop(columns=zero_cols)\n",
    "print(\"Dropped redundant rebound columns and renamed others for clarity.\")\n",
    "\n",
    "# Drop the 'divID' column as it contains only empty strings and does not add useful information\n",
    "teams = teams.drop(columns=['divID'])\n",
    "print(\"Dropped 'divID' column as it contains no information.\")\n",
    "\n",
    "# Drop the 'seeded' column as it contains only zero values\n",
    "teams = teams.drop(columns=['seeded'])\n",
    "print(\"Dropped 'seeded' column as it contains only zero values.\")\n",
    "\n",
    "def calculate_playoff_score(row):\n",
    "    if row['finals'] == 'W':\n",
    "        return 4  # Won the championship\n",
    "    elif row['finals'] == 'L':\n",
    "        return 3  # Lost in the finals\n",
    "    elif row['semis'] == 'L':\n",
    "        return 2  # Lost in the semifinals\n",
    "    elif row['firstRound'] == 'L':\n",
    "        return 1  # Lost in the first round\n",
    "    else:\n",
    "        return 0  # Did not make the playoffs\n",
    "\n",
    "# Apply the function to each row to create the playoff_progression_score\n",
    "teams['playoff_progression_score'] = teams.apply(calculate_playoff_score, axis=1)\n",
    "\n",
    "# Drop the original 'firstRound', 'semis', and 'finals' columns as they are now redundant\n",
    "teams = teams.drop(columns=['firstRound', 'semis', 'finals'])\n",
    "\n",
    "# Drop columns that don't add predictive value\n",
    "# 'lgID': Contains only \"WNBA\" for every row, so it provides no additional information.\n",
    "# 'franchID': Redundant identifier, as 'tmID' already identifies each team uniquely.\n",
    "# 'confID': Lacks value without conference-specific qualification/matchup data.\n",
    "# 'name': Purely descriptive and irrelevant to playoff predictions.\n",
    "# 'arena': Also descriptive and does not impact playoff qualification.\n",
    "teams = teams.drop(columns=['lgID', 'franchID', 'confID', 'name', 'arena'])\n",
    "print(\"Dropped 'lgID', 'franchID', 'confID', 'name', and 'arena' as they are irrelevant for predictive modeling.\")\n",
    "\n",
    "# DETECTION OF MISSING VALUES\n",
    "missing_values = detect_missing_values(teams)\n",
    "print(\"\\nMissing Values\")\n",
    "print(missing_values)\n",
    "\n",
    "# DETECTION OF DUPLICATES\n",
    "duplicate_rows = detect_duplicates(teams)\n",
    "print(\"\\nDuplicates\")\n",
    "print(duplicate_rows)\n",
    "\n",
    "# DETECTION OF OUTLIERS\n",
    "# Select numerical columns only\n",
    "numeric_columns = teams.select_dtypes(include=['float64', 'int64']).columns\n",
    "\n",
    "#for column in numeric_columns:\n",
    "#    plt.figure(figsize=(8, 4))\n",
    "#    sns.boxplot(x=teams[column])\n",
    "#    plt.title(f'Box Plot of {column}')\n",
    "#    plt.show()\n",
    "#\n",
    "#outliers = detect_outliers(teams, numeric_columns)\n",
    "#\n",
    "#for col, outlier_data in outliers.items():\n",
    "#    print(f\"Outliers in {col}:\\n{outlier_data}\\n\")\n",
    "\n",
    "## Plot for 'next_season_playoff' column\n",
    "#plt.figure(figsize=(6, 4))\n",
    "#plt.bar(next_playoff_counts.index, next_playoff_counts.values)\n",
    "#plt.title('Data Balance in Next Season Playoff')\n",
    "#plt.xlabel('Next Season Playoff (0 = No, 1 = Yes)')\n",
    "#plt.ylabel('Count')\n",
    "#plt.xticks([0, 1])\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 597,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Correlation between offensive statistics\n",
    "## -------------------------\n",
    "#\n",
    "#o_stats = teams.filter(regex='^(o_)')\n",
    "#\n",
    "#corr_matrix = o_stats.corr()\n",
    "#\n",
    "#plt.figure(figsize=(14, 12))\n",
    "#\n",
    "#cmap = sns.color_palette(\"mako\", as_cmap=True)\n",
    "#\n",
    "#sns.heatmap(corr_matrix, cmap=cmap, vmax=1.0, vmin=-1.0, center=0,\n",
    "#            square=True, linewidths=.5, annot=True, fmt=\".2f\", annot_kws={\"size\":8})\n",
    "#\n",
    "#plt.title('Correlation Heatmap of Performance Statistics', fontsize=18)\n",
    "#plt.xticks(rotation=45, ha='right', fontsize=10)\n",
    "#plt.yticks(fontsize=10)\n",
    "#plt.tight_layout()\n",
    "#\n",
    "## Display the heatmap\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 598,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Correlation between defensive statistics\n",
    "## -------------------------\n",
    "#\n",
    "#d_stats = teams.filter(regex='^(d_)')\n",
    "#\n",
    "#corr_matrix = d_stats.corr()\n",
    "#\n",
    "#plt.figure(figsize=(14, 12))\n",
    "#\n",
    "#cmap = sns.color_palette(\"coolwarm\", as_cmap=True)\n",
    "#\n",
    "#sns.heatmap(corr_matrix, cmap=cmap, vmax=1.0, vmin=-1.0, center=0,\n",
    "#            square=True, linewidths=.5, annot=True, fmt=\".2f\", annot_kws={\"size\":8})\n",
    "#\n",
    "#plt.title('Correlation Heatmap of Performance Statistics', fontsize=18)\n",
    "#plt.xticks(rotation=45, ha='right', fontsize=10)\n",
    "#plt.yticks(fontsize=10)\n",
    "#plt.tight_layout()\n",
    "#\n",
    "## Display the heatmap\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 599,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Compute correlation between offensive and defensive stats\n",
    "#o_stats = teams.filter(regex='^(o_)')\n",
    "#d_stats = teams.filter(regex='^(d_)')\n",
    "#\n",
    "#combined_stats = pd.concat([o_stats, d_stats], axis=1)\n",
    "#\n",
    "#corr_matrix = combined_stats.corr()\n",
    "#\n",
    "#o_d_corr_matrix = corr_matrix.loc[o_stats.columns, d_stats.columns]\n",
    "#\n",
    "#plt.figure(figsize=(12, 10))\n",
    "#sns.heatmap(o_d_corr_matrix, annot=True, fmt=\".2f\", cmap=\"coolwarm\", center=0,\n",
    "#            linewidths=.5, square=True, cbar_kws={\"shrink\": .75})\n",
    "#\n",
    "#plt.title(\"Correlation Between Offensive and Defensive Statistics\")\n",
    "#plt.xticks(rotation=45, ha='right', fontsize=10)\n",
    "#plt.yticks(fontsize=10)\n",
    "#plt.tight_layout()\n",
    "#\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 600,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate statistics for the new dataframe\n",
    "dataset = pd.DataFrame()\n",
    "\n",
    "# Adding team statistics\n",
    "#dataset['Playoff'] = teams['playoff']\n",
    "dataset['Rank'] = teams['rank']\n",
    "dataset['PlayoffProgScore'] = teams['playoff_progression_score']\n",
    "dataset['GP'] = teams['GP']\n",
    "dataset['W'] = teams['won']\n",
    "dataset['L'] = teams['lost']\n",
    "dataset['WIN%'] = 100 * (teams['won'] / teams['GP'])\n",
    "dataset['MIN'] = teams['min']\n",
    "dataset['PTS'] = teams['o_pts']\n",
    "dataset['FGM'] = teams['o_fgm']\n",
    "dataset['FGA'] = teams['o_fga']\n",
    "dataset['FG%'] = 100 * (teams['o_fgm'] / teams['o_fga'])\n",
    "dataset['3PM'] = teams['o_3pm']\n",
    "dataset['3PA'] = teams['o_3pa']\n",
    "dataset['3P%'] = 100 * (teams['o_3pm'] / teams['o_3pa'])\n",
    "dataset['FTM'] = teams['o_ftm']\n",
    "dataset['FTA'] = teams['o_fta']\n",
    "dataset['FT%'] = 100 * (teams['o_ftm'] / teams['o_fta'])\n",
    "dataset['OREB'] = teams['o_oreb']\n",
    "dataset['DREB'] = teams['o_dreb']\n",
    "dataset['REB'] = teams['o_reb']\n",
    "dataset['AST'] = teams['o_asts']\n",
    "dataset['TOV'] = teams['o_to']\n",
    "dataset['STL'] = teams['o_stl']\n",
    "dataset['BLK'] = teams['o_blk']\n",
    "dataset['BLKA'] = teams['d_blk']\n",
    "dataset['PF'] = teams['o_pf']\n",
    "dataset['PFD'] = teams['d_pf']\n",
    "\n",
    "# Advanced\n",
    "dataset['POSS'] = 0.5 * (\n",
    "    (teams['o_fga'] + 0.4 * teams['o_fta'] -\n",
    "     1.07 * (teams['o_oreb'] / (teams['o_oreb'] + teams['d_dreb'])) *\n",
    "     (teams['o_fga'] - teams['o_fgm']) + teams['o_to']) +\n",
    "    (teams['d_fga'] + 0.4 * teams['d_fta'] -\n",
    "     1.07 * (teams['d_oreb'] / (teams['d_oreb'] + teams['o_dreb'])) *\n",
    "     (teams['d_fga'] - teams['d_fgm']) + teams['d_to'])\n",
    ")\n",
    "dataset['OFFRTG'] = 100 * (teams['o_pts'] / dataset['POSS'])\n",
    "dataset['DEFRTG'] = 100 * (teams['d_pts'] / dataset['POSS'])\n",
    "dataset['NETRTG'] = dataset['OFFRTG'] - dataset['DEFRTG']\n",
    "dataset['AST/TO'] = teams['o_asts'] / teams['o_to']\n",
    "dataset['AST RATIO'] = (teams['o_asts'] * 100) / dataset['POSS']\n",
    "dataset['OREB%'] = (\n",
    "    100 * (teams['o_oreb'] * (dataset['MIN'] / 5)) / \n",
    "    (dataset['MIN'] * (teams['o_oreb'] + teams['d_dreb']))\n",
    ")\n",
    "dataset['DREB%'] = (\n",
    "    100 * (teams['o_dreb'] * (dataset['MIN'] / 5)) / \n",
    "    (dataset['MIN'] * (teams['o_dreb'] + teams['d_oreb']))\n",
    ")\n",
    "dataset['REB%'] = (\n",
    "    100 * (teams['o_reb'] * (dataset['MIN'] / 5)) / \n",
    "    (dataset['MIN'] * (teams['o_reb'] + teams['d_reb']))\n",
    ")\n",
    "dataset['TOV%'] = 100 * teams['o_to'] / (\n",
    "    teams['o_fga'] + 0.44 * teams['o_fta'] + teams['o_to']\n",
    ")\n",
    "dataset['EFG%'] = 100 * ((teams['o_fgm'] + (0.5 * teams['o_3pm'])) / teams['o_fga'])\n",
    "dataset['TS%'] = 100 * (teams['o_pts'] / (2 * (teams['o_fga'] + 0.44 * teams['o_fta'])))\n",
    "\n",
    "OPPPOSS = 0.5 * (\n",
    "    (teams['d_fga'] + 0.4 * teams['d_fta'] -\n",
    "     1.07 * (teams['d_oreb'] / (teams['d_oreb'] + teams['o_dreb'])) *\n",
    "     (teams['d_fga'] - teams['d_fgm']) + teams['d_to']) +\n",
    "    (teams['o_fga'] + 0.4 * teams['o_fta'] -\n",
    "     1.07 * (teams['o_oreb'] / (teams['o_oreb'] + teams['d_dreb'])) *\n",
    "     (teams['o_fga'] - teams['o_fgm']) + teams['o_to'])\n",
    ")\n",
    "dataset['PACE'] = 40 * ((dataset['POSS'] + OPPPOSS) / (2 * (dataset['MIN'] / 5)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 601,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Label\n",
    "teams = teams.sort_values(by=['tmID', 'year']).reset_index(drop=True)\n",
    "dataset['PlayoffNextSeason'] = teams.groupby('tmID')['playoff'].shift(-1)\n",
    "dataset = dataset.dropna(subset=['PlayoffNextSeason'])\n",
    "dataset['PlayoffNextSeason'] = dataset['PlayoffNextSeason'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 602,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PlayoffNextSeason\n",
       "1    71\n",
       "0    51\n",
       "Name: count, dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "PlayoffNextSeason\n",
       "1    71\n",
       "0    71\n",
       "Name: count, dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Separate features and target variable\n",
    "X = dataset.drop(columns=['PlayoffNextSeason'])\n",
    "y = dataset['PlayoffNextSeason']\n",
    "\n",
    "display(y.value_counts())\n",
    "\n",
    "# Apply SMOTE to balance the classes\n",
    "smote = SMOTE(random_state=42)\n",
    "X_resampled, y_resampled = smote.fit_resample(X, y)\n",
    "\n",
    "# Verify the new class distribution after applying SMOTE\n",
    "display(y_resampled.value_counts())\n",
    "\n",
    "# Merge resampled data into a new DataFrame\n",
    "balanced_dataset = pd.DataFrame(X_resampled, columns=X.columns)\n",
    "balanced_dataset['PlayoffNextSeason'] = y_resampled\n",
    "\n",
    "dataset = balanced_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cells bellow are to be used when cleaning, transforming and analyzing the remaining datasets. For now we are only studying teams.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 603,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n# Load the teams_post data\\nteams_post = pd.read_csv(\"./data/teams_post.csv\")\\n\\n# Drop \\'lgID\\' column as it contains only \"WNBA\" for every row\\nteams_post = teams_post.drop(columns=[\\'lgID\\'])\\n\\n# Detect and drop duplicates\\nduplicates = teams_post[teams_post.duplicated()]\\nif not duplicates.empty:\\n    print(\"Duplicates detected. Removing duplicate rows.\")\\n    teams_post = teams_post.drop_duplicates()\\nelse:\\n    print(\"No duplicates found.\")\\n\\n# Display a sample of the dataframe to verify changes\\ndisplay(teams_post.head())\\n\\n# Store cleaned csv\\nteams_post.to_csv(\\'./cleaned_data/teams_post.csv\\', index=False)\\n'"
      ]
     },
     "execution_count": 603,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "# Load the teams_post data\n",
    "teams_post = pd.read_csv(\"./data/teams_post.csv\")\n",
    "\n",
    "# Drop 'lgID' column as it contains only \"WNBA\" for every row\n",
    "teams_post = teams_post.drop(columns=['lgID'])\n",
    "\n",
    "# Detect and drop duplicates\n",
    "duplicates = teams_post[teams_post.duplicated()]\n",
    "if not duplicates.empty:\n",
    "    print(\"Duplicates detected. Removing duplicate rows.\")\n",
    "    teams_post = teams_post.drop_duplicates()\n",
    "else:\n",
    "    print(\"No duplicates found.\")\n",
    "\n",
    "# Display a sample of the dataframe to verify changes\n",
    "display(teams_post.head())\n",
    "\n",
    "# Store cleaned csv\n",
    "teams_post.to_csv('./cleaned_data/teams_post.csv', index=False)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 604,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n# Load the series_post data\\nseries_post = pd.read_csv(\"./data/series_post.csv\")\\n\\n# Drop \\'lgIDWinner\\' and \\'lgIDLoser\\' columns as they contain only \"WNBA\" and add no value\\nseries_post = series_post.drop(columns=[\\'lgIDWinner\\', \\'lgIDLoser\\'])\\n\\n# Detect and drop duplicates\\nduplicates = series_post[series_post.duplicated()]\\nif not duplicates.empty:\\n    print(\"Duplicates detected. Removing duplicate rows.\")\\n    series_post = series_post.drop_duplicates()\\nelse:\\n    print(\"No duplicates found.\")\\n\\n# Display a sample of the dataframe to verify changes\\ndisplay(series_post.head())\\n\\n# Store cleaned csv\\nseries_post.to_csv(\\'./cleaned_data/series_post.csv\\', index=False)\\n'"
      ]
     },
     "execution_count": 604,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "# Load the series_post data\n",
    "series_post = pd.read_csv(\"./data/series_post.csv\")\n",
    "\n",
    "# Drop 'lgIDWinner' and 'lgIDLoser' columns as they contain only \"WNBA\" and add no value\n",
    "series_post = series_post.drop(columns=['lgIDWinner', 'lgIDLoser'])\n",
    "\n",
    "# Detect and drop duplicates\n",
    "duplicates = series_post[series_post.duplicated()]\n",
    "if not duplicates.empty:\n",
    "    print(\"Duplicates detected. Removing duplicate rows.\")\n",
    "    series_post = series_post.drop_duplicates()\n",
    "else:\n",
    "    print(\"No duplicates found.\")\n",
    "\n",
    "# Display a sample of the dataframe to verify changes\n",
    "display(series_post.head())\n",
    "\n",
    "# Store cleaned csv\n",
    "series_post.to_csv('./cleaned_data/series_post.csv', index=False)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 605,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No duplicates found in player awards.\n",
      "No duplicates found in coach awards.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>playerID</th>\n",
       "      <th>award</th>\n",
       "      <th>year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>thompti01w</td>\n",
       "      <td>All-Star Game MVP</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>leslili01w</td>\n",
       "      <td>All-Star Game MVP</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>leslili01w</td>\n",
       "      <td>All-Star Game MVP</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>teaslni01w</td>\n",
       "      <td>All-Star Game MVP</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>swoopsh01w</td>\n",
       "      <td>All-Star Game MVP</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     playerID              award  year\n",
       "0  thompti01w  All-Star Game MVP     1\n",
       "1  leslili01w  All-Star Game MVP     2\n",
       "2  leslili01w  All-Star Game MVP     3\n",
       "3  teaslni01w  All-Star Game MVP     4\n",
       "4  swoopsh01w  All-Star Game MVP     6"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>coachID</th>\n",
       "      <th>award</th>\n",
       "      <th>year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>coopemi01w</td>\n",
       "      <td>Coach of the Year</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>hugheda99w</td>\n",
       "      <td>Coach of the Year</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>stanlma99w</td>\n",
       "      <td>Coach of the Year</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>laimbbi01w</td>\n",
       "      <td>Coach of the Year</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>mcconsu01w</td>\n",
       "      <td>Coach of the Year</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       coachID              award  year\n",
       "8   coopemi01w  Coach of the Year     1\n",
       "9   hugheda99w  Coach of the Year     2\n",
       "10  stanlma99w  Coach of the Year     3\n",
       "11  laimbbi01w  Coach of the Year     4\n",
       "12  mcconsu01w  Coach of the Year     5"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load the awards_players data\n",
    "awards_players = pd.read_csv(\"./data/awards_players.csv\")\n",
    "\n",
    "# Drop 'lgID' column as it provides no unique value\n",
    "awards_players = awards_players.drop(columns=['lgID'])\n",
    "\n",
    "# Separate dataframes for player awards and coach awards\n",
    "player_awards = awards_players[~awards_players['award'].str.contains(\"Coach\")].copy()\n",
    "coach_awards = awards_players[awards_players['award'].str.contains(\"Coach\")].copy()\n",
    "\n",
    "# Standardize award names\n",
    "award_name_mapping = {\n",
    "    \"Kim Perrot Sportsmanship\": \"Kim Perrot Sportsmanship Award\",\n",
    "    \"Kim Perrot Sportsmanship Award\": \"Kim Perrot Sportsmanship Award\",\n",
    "    \"All-Star Game Most Valuable Player\": \"All-Star Game MVP\",\n",
    "    \"Most Valuable Player\": \"MVP\",\n",
    "    \"WNBA Finals Most Valuable Player\": \"Finals MVP\",\n",
    "    \"Sixth Woman of the Year\": \"6th Woman of the Year\",\n",
    "    \"WNBA All-Decade Team\": \"All-Decade Team\",\n",
    "    \"WNBA All Decade Team Honorable Mention\": \"All-Decade Team Honorable Mention\"\n",
    "}\n",
    "\n",
    "player_awards.loc[:, 'award'] = player_awards['award'].map(award_name_mapping).fillna(player_awards['award'])\n",
    "coach_awards.loc[:, 'award'] = coach_awards['award'].map(award_name_mapping).fillna(coach_awards['award'])\n",
    "\n",
    "# Detect and drop duplicates\n",
    "duplicates_player = player_awards[player_awards.duplicated()]\n",
    "if not duplicates_player.empty:\n",
    "    print(\"Duplicates detected in player awards. Removing duplicate rows.\")\n",
    "    player_awards = player_awards.drop_duplicates()\n",
    "else:\n",
    "    print(\"No duplicates found in player awards.\")\n",
    "\n",
    "duplicates_coach = coach_awards[coach_awards.duplicated()]\n",
    "if not duplicates_coach.empty:\n",
    "    print(\"Duplicates detected in coach awards. Removing duplicate rows.\")\n",
    "    coach_awards = coach_awards.drop_duplicates()\n",
    "else:\n",
    "    print(\"No duplicates found in coach awards.\")\n",
    "\n",
    "# Swap 'playerID' for 'coachID' in coach awards\n",
    "coach_awards = coach_awards.rename(columns={'playerID': 'coachID'})\n",
    "\n",
    "# Display samples of both dataframes to verify transformations\n",
    "display(player_awards.head())\n",
    "display(coach_awards.head())\n",
    "\n",
    "# Store cleaned csvs\n",
    "player_awards.to_csv('./cleaned_data/player_awards.csv', index=False)\n",
    "coach_awards.to_csv('./cleaned_data/coach_awards.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 606,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No duplicates found.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>coachID</th>\n",
       "      <th>year</th>\n",
       "      <th>tmID</th>\n",
       "      <th>stint</th>\n",
       "      <th>won</th>\n",
       "      <th>lost</th>\n",
       "      <th>post_wins</th>\n",
       "      <th>post_losses</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>adamsmi01w</td>\n",
       "      <td>5</td>\n",
       "      <td>WAS</td>\n",
       "      <td>0</td>\n",
       "      <td>17</td>\n",
       "      <td>17</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>adubari99w</td>\n",
       "      <td>1</td>\n",
       "      <td>NYL</td>\n",
       "      <td>0</td>\n",
       "      <td>20</td>\n",
       "      <td>12</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>adubari99w</td>\n",
       "      <td>2</td>\n",
       "      <td>NYL</td>\n",
       "      <td>0</td>\n",
       "      <td>21</td>\n",
       "      <td>11</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>adubari99w</td>\n",
       "      <td>3</td>\n",
       "      <td>NYL</td>\n",
       "      <td>0</td>\n",
       "      <td>18</td>\n",
       "      <td>14</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>adubari99w</td>\n",
       "      <td>4</td>\n",
       "      <td>NYL</td>\n",
       "      <td>0</td>\n",
       "      <td>16</td>\n",
       "      <td>18</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      coachID  year tmID  stint  won  lost  post_wins  post_losses\n",
       "0  adamsmi01w     5  WAS      0   17    17          1            2\n",
       "1  adubari99w     1  NYL      0   20    12          4            3\n",
       "2  adubari99w     2  NYL      0   21    11          3            3\n",
       "3  adubari99w     3  NYL      0   18    14          4            4\n",
       "4  adubari99w     4  NYL      0   16    18          0            0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load the coaches data\n",
    "coaches = pd.read_csv(\"./data/coaches.csv\")\n",
    "\n",
    "# Drop 'lgID' as it is only \"WNBA\" and provides no unique value\n",
    "coaches = coaches.drop(columns=['lgID'])\n",
    "\n",
    "# Detect and drop duplicates\n",
    "duplicates = coaches[coaches.duplicated()]\n",
    "if not duplicates.empty:\n",
    "    print(\"Duplicates detected. Removing duplicate rows.\")\n",
    "    coaches = coaches.drop_duplicates()\n",
    "else:\n",
    "    print(\"No duplicates found.\")\n",
    "\n",
    "# Display a sample of the dataframe to verify changes\n",
    "display(coaches.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding Coach to the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 607,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      coachID  year  won  lost  post_wins  post_losses\n",
      "0  adamsmi01w     5   17    17          1            2\n",
      "1  adubari99w     1   20    12          4            3\n",
      "2  adubari99w     2   21    11          3            3\n",
      "3  adubari99w     3   18    14          4            4\n",
      "4  adubari99w     4   16    18          0            0\n"
     ]
    }
   ],
   "source": [
    "# Aggregate data by coachID and year (ignoring stint and teamID)\n",
    "coaches_agg = coaches.groupby(['coachID', 'year']).agg({\n",
    "    'won': 'sum',\n",
    "    'lost': 'sum',\n",
    "    'post_wins': 'sum',\n",
    "    'post_losses': 'sum'\n",
    "}).reset_index()\n",
    "\n",
    "# Preview the aggregated data\n",
    "print(coaches_agg.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 608,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      coachID  year  championship  win_ratio  post_win_ratio  COTY\n",
      "0  adamsmi01w     5             0     0.5000          0.3333     0\n",
      "1  adubari99w     1             0     0.6250          0.5714     0\n",
      "2  adubari99w     2             0     0.6562          0.5000     0\n",
      "3  adubari99w     3             0     0.5625          0.5000     0\n",
      "4  adubari99w     4             0     0.4706          0.0000     0\n"
     ]
    }
   ],
   "source": [
    "# Find the coach with the maximum post_wins for each year\n",
    "max_post_wins_per_year = coaches_agg.loc[coaches_agg.groupby('year')['post_wins'].idxmax()][['year', 'coachID', 'post_wins']]\n",
    "max_post_wins_per_year = max_post_wins_per_year.rename(columns={'post_wins': 'max_post_wins'})\n",
    "\n",
    "# Merge this information back with the original DataFrame\n",
    "coaches_agg = coaches_agg.merge(max_post_wins_per_year, on=['year', 'coachID'], how='left')\n",
    "\n",
    "# Set the championship indicator\n",
    "coaches_agg['championship'] = 0\n",
    "coaches_agg.loc[coaches_agg.set_index(['year', 'coachID']).index.isin(max_post_wins_per_year.set_index(['year', 'coachID']).index) & (coaches_agg['post_wins'] >= 6), 'championship'] = 1\n",
    "\n",
    "# Drop the temporary max_post_wins column\n",
    "coaches_agg.drop(columns=['max_post_wins'], inplace=True)\n",
    "\n",
    "# Recalculate metrics\n",
    "coaches_agg['win_ratio'] = (coaches_agg['won'] / (coaches_agg['won'] + coaches_agg['lost'])).round(4)\n",
    "coaches_agg['post_win_ratio'] = (coaches_agg['post_wins'] / (coaches_agg['post_wins'] + coaches_agg['post_losses'])).round(4)\n",
    "\n",
    "# If a coach has not reached playoffs, post_win% will be NaN. Fill these with 0.\n",
    "coaches_agg['post_win_ratio'] = coaches_agg['post_win_ratio'].fillna(0)\n",
    "\n",
    "# Merge with Awards Data for Coach of the Year\n",
    "coaches_agg = coaches_agg.merge(\n",
    "    coach_awards[['coachID', 'year', 'award']], \n",
    "    on=['coachID', 'year'], \n",
    "    how='left'\n",
    ")\n",
    "coaches_agg['COTY'] = np.where(coaches_agg['award'] == 'Coach of the Year', 1, 0)\n",
    "coaches_agg.drop(columns=['award', 'won', 'lost'], inplace=True)\n",
    "\n",
    "#Normalize features if needed\n",
    "coaches_agg['win_ratio'] = coaches_agg['win_ratio'].clip(0, 1)  # Ensure the ratio is between 0 and 1\n",
    "coaches_agg['post_win_ratio'] = coaches_agg['post_win_ratio'].clip(0, 1)  # Ensure the ratio is between 0 and 1\n",
    "\n",
    "# Drop the temporary columns\n",
    "coaches_agg.drop(columns=['post_wins', 'post_losses'], inplace=True)\n",
    "\n",
    "# Preview updated data\n",
    "print(coaches_agg.head())\n",
    "\n",
    "# Store cleaned csv\n",
    "coaches_agg.to_csv('./cleaned_data/coaches.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 609,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n# Load the players_teams data\\nplayers_teams = pd.read_csv(\"./data/players_teams.csv\")\\n\\n# Drop \\'lgID\\' as it contains only \"WNBA\" and provides no unique value\\nplayers_teams = players_teams.drop(columns=[\\'lgID\\'])\\n\\n# Detect and drop duplicates\\nduplicates = players_teams[players_teams.duplicated()]\\nif not duplicates.empty:\\n    print(\"Duplicates detected. Removing duplicate rows.\")\\n    players_teams = players_teams.drop_duplicates()\\nelse:\\n    print(\"No duplicates found.\")\\n\\n# Display a sample of the dataframe to verify changes\\ndisplay(players_teams.head())\\n\\n# Store cleaned csv\\nplayers_teams.to_csv(\\'./cleaned_data/players_teams.csv\\', index=False)\\n'"
      ]
     },
     "execution_count": 609,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "# Load the players_teams data\n",
    "players_teams = pd.read_csv(\"./data/players_teams.csv\")\n",
    "\n",
    "# Drop 'lgID' as it contains only \"WNBA\" and provides no unique value\n",
    "players_teams = players_teams.drop(columns=['lgID'])\n",
    "\n",
    "# Detect and drop duplicates\n",
    "duplicates = players_teams[players_teams.duplicated()]\n",
    "if not duplicates.empty:\n",
    "    print(\"Duplicates detected. Removing duplicate rows.\")\n",
    "    players_teams = players_teams.drop_duplicates()\n",
    "else:\n",
    "    print(\"No duplicates found.\")\n",
    "\n",
    "# Display a sample of the dataframe to verify changes\n",
    "display(players_teams.head())\n",
    "\n",
    "# Store cleaned csv\n",
    "players_teams.to_csv('./cleaned_data/players_teams.csv', index=False)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 610,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n# Load the players data\\nplayers = pd.read_csv(\"./data/players.csv\")\\n\\n# Filter out rows in players that do not have corresponding playerIDs in players_teams\\nvalid_player_ids = players_teams[\\'playerID\\'].unique()\\nplayers = players[players[\\'bioID\\'].isin(valid_player_ids)]\\n\\n# Show that all values in firstseason and lastseason are \\'0\\'\\nfirstseason_all_zero = (players[\\'firstseason\\'] == 0).all()\\nlastseason_all_zero = (players[\\'lastseason\\'] == 0).all()\\n\\nprint(\"All values in \\'firstseason\\' are 0:\", firstseason_all_zero)\\nprint(\"All values in \\'lastseason\\' are 0:\", lastseason_all_zero)\\n\\n# Show the only valida player with a registered Death Date\\nnon_zero_death_dates = players[players[\\'deathDate\\'] != \"0000-00-00\"]\\ndisplay(non_zero_death_dates.head())\\n\\n# Even though there is 1 registered Death Date, it really doesn\\'t add anything. Birth Date is kept, for potential aging information.\\nplayers = players.drop(columns=[\\'firstseason\\', \\'lastseason\\', \\'deathDate\\'])\\nprint(\"Dropped \\'firstseason\\', \\'lastseason\\', and \\'deathDate\\' columns as they contain only irrelevant values.\")\\n\\n# Detect and drop duplicates\\nduplicates = players[players.duplicated()]\\nif not duplicates.empty:\\n    print(\"Duplicates detected. Removing duplicate rows.\")\\n    players = players.drop_duplicates()\\nelse:\\n    print(\"No duplicates found.\")\\n\\n# Display a sample of the dataframe to verify changes\\ndisplay(players.head())\\n\\n# Store cleaned csv\\nplayers.to_csv(\\'./cleaned_data/players.csv\\', index=False)\\n'"
      ]
     },
     "execution_count": 610,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "# Load the players data\n",
    "players = pd.read_csv(\"./data/players.csv\")\n",
    "\n",
    "# Filter out rows in players that do not have corresponding playerIDs in players_teams\n",
    "valid_player_ids = players_teams['playerID'].unique()\n",
    "players = players[players['bioID'].isin(valid_player_ids)]\n",
    "\n",
    "# Show that all values in firstseason and lastseason are '0'\n",
    "firstseason_all_zero = (players['firstseason'] == 0).all()\n",
    "lastseason_all_zero = (players['lastseason'] == 0).all()\n",
    "\n",
    "print(\"All values in 'firstseason' are 0:\", firstseason_all_zero)\n",
    "print(\"All values in 'lastseason' are 0:\", lastseason_all_zero)\n",
    "\n",
    "# Show the only valida player with a registered Death Date\n",
    "non_zero_death_dates = players[players['deathDate'] != \"0000-00-00\"]\n",
    "display(non_zero_death_dates.head())\n",
    "\n",
    "# Even though there is 1 registered Death Date, it really doesn't add anything. Birth Date is kept, for potential aging information.\n",
    "players = players.drop(columns=['firstseason', 'lastseason', 'deathDate'])\n",
    "print(\"Dropped 'firstseason', 'lastseason', and 'deathDate' columns as they contain only irrelevant values.\")\n",
    "\n",
    "# Detect and drop duplicates\n",
    "duplicates = players[players.duplicated()]\n",
    "if not duplicates.empty:\n",
    "    print(\"Duplicates detected. Removing duplicate rows.\")\n",
    "    players = players.drop_duplicates()\n",
    "else:\n",
    "    print(\"No duplicates found.\")\n",
    "\n",
    "# Display a sample of the dataframe to verify changes\n",
    "display(players.head())\n",
    "\n",
    "# Store cleaned csv\n",
    "players.to_csv('./cleaned_data/players.csv', index=False)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 611,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed dataset saved to ./cleaned_data/dataset.csv\n"
     ]
    }
   ],
   "source": [
    "# Save the new dataset to a CSV file\n",
    "output_file_path = './cleaned_data/dataset.csv'\n",
    "dataset.to_csv(output_file_path, index=False)\n",
    "\n",
    "print(f\"Processed dataset saved to {output_file_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
