{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.feature_selection import mutual_info_classif\n",
    "from sklearn.model_selection import StratifiedKFold, cross_validate\n",
    "from sklearn.metrics import make_scorer, confusion_matrix, roc_curve, auc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training\n",
    "\n",
    "In this notebook we will be looking to train the XGBoost model and see how good our features are for the model, checking statistics like accuracy, F-1 score, etc.\n",
    "\n",
    "First, let's separate our target and our features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Load the teams data\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m dataset \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./cleaned_data/dataset.csv\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      4\u001b[0m label \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPlayoffNextSeason\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m      5\u001b[0m features \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m      6\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCumulativePlayoffProgScore\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m3P\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFT\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m      7\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mOFFRTG\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDEFRTG\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAST RATIO\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m      8\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mREB\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTOV\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPACE\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAvgPIE_NextYearPlayers\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPerformance_NextYearCoach\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m      9\u001b[0m ]\n",
      "\u001b[1;31mNameError\u001b[0m: name 'pd' is not defined"
     ]
    }
   ],
   "source": [
    "# Load the teams data\n",
    "dataset = pd.read_csv('./cleaned_data/dataset.csv')\n",
    "\n",
    "label = 'PlayoffNextSeason'\n",
    "features = [\n",
    "    'CumulativePlayoffProgScore', '3P%', 'FT%',\n",
    "    'OFFRTG', 'DEFRTG', 'AST RATIO',\n",
    "    'REB%', 'TOV%', 'PACE', 'AvgPIE_NextYearPlayers', 'Performance_NextYearCoach'\n",
    "]\n",
    "\n",
    "# Prepare feature and label data\n",
    "X = dataset[features]\n",
    "y = dataset[label]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then plotted a correlation matrix and mutual information classification table to distinguish our most important features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================\n",
    "# Display Initial Correlation Matrix\n",
    "# ============================\n",
    "\n",
    "# Calculate and plot the correlation matrix\n",
    "correlation_matrix = dataset[features + [label]].corr()\n",
    "plt.figure(figsize=(20, 18))\n",
    "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', center=0)\n",
    "plt.title('Feature Correlation Matrix (Before Selection)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================\n",
    "# Display Initial Mutual Information Scores\n",
    "# ============================\n",
    "\n",
    "# Calculate mutual information\n",
    "mi_scores = mutual_info_classif(X, y, discrete_features='auto', random_state=42)\n",
    "mi_scores = pd.Series(mi_scores, index=X.columns, name='MI Scores')\n",
    "mi_scores = mi_scores.sort_values(ascending=False)\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.barplot(x=mi_scores.values, y=mi_scores.index)\n",
    "plt.title('Mutual Information Scores (Before Selection)')\n",
    "plt.xlabel('Mutual Information')\n",
    "plt.ylabel('Features')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we identified which features had a correlation above 0.9 with the target variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "# ============================\n",
    "# Correlation Filtering\n",
    "# ============================\n",
    "\n",
    "# Calculate the correlation matrix\n",
    "correlation_matrix = X.corr()\n",
    "correlation_threshold = 0.9\n",
    "correlated_features = set()\n",
    "\n",
    "# Identify features with correlation above the threshold\n",
    "for i in range(len(correlation_matrix.columns)):\n",
    "    for j in range(i):\n",
    "        if abs(correlation_matrix.iloc[i, j]) > correlation_threshold:\n",
    "            correlated_features.add(correlation_matrix.columns[i])\n",
    "\n",
    "# Keep only non-correlated features\n",
    "X = X.drop(columns=correlated_features)\n",
    "print(\"Remaining features correlation filtering:\", list(X))\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also filtered the features by checking mutual information scores above a certain threshold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "# ============================\n",
    "# Mutual Information Filtering\n",
    "# ============================\n",
    "\n",
    "# Calculate mutual information\n",
    "mutual_info = mutual_info_classif(X, y, discrete_features='auto', random_state=42)\n",
    "mutual_info_series = pd.Series(mutual_info, index=X.columns)\n",
    "\n",
    "# Set a threshold for mutual information\n",
    "mi_threshold = 0.02\n",
    "selected_features_mi = mutual_info_series[mutual_info_series > mi_threshold].index.tolist()\n",
    "\n",
    "# Filter the dataset based on selected features\n",
    "X = X[selected_features_mi]\n",
    "print(\"Remaining features after mutual information filtering:\", selected_features_mi)\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Defining the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================\n",
    "# Define the XGBoost model\n",
    "# ============================\n",
    "\n",
    "def create_model(trial):\n",
    "    return XGBClassifier(\n",
    "        random_state=42,\n",
    "        eval_metric='logloss',\n",
    "        learning_rate=trial.suggest_float('learning_rate', 0.01, 0.3),\n",
    "        n_estimators=trial.suggest_int('n_estimators', 50, 500),\n",
    "        max_depth=trial.suggest_int('max_depth', 3, 10),\n",
    "        min_child_weight=trial.suggest_int('min_child_weight', 1, 10),\n",
    "        subsample=trial.suggest_float('subsample', 0.5, 1.0),\n",
    "        colsample_bytree=trial.suggest_float('colsample_bytree', 0.5, 1.0),\n",
    "        gamma=trial.suggest_float('gamma', 0, 5),\n",
    "        reg_alpha=trial.suggest_float('reg_alpha', 0, 1),\n",
    "        reg_lambda=trial.suggest_float('reg_lambda', 0, 1),\n",
    "        scale_pos_weight=trial.suggest_float('scale_pos_weight', 1, 100)\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After defining the model, `StratifiedKFold` with K=10 was used to mantain class distribution in each fold. We also defined a custom scorer for specificity, which is the true negative rate. This ensures that the cross-validation process is robust."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use StratifiedKFold to maintain class distribution in each fold\n",
    "skf = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)\n",
    "\n",
    "# Define specificity scorer\n",
    "def specificity(y_true, y_pred):\n",
    "    tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()\n",
    "    return tn / (tn + fp)\n",
    "\n",
    "specificity_scorer = make_scorer(specificity)\n",
    "\n",
    "# Define the scoring metrics\n",
    "scoring = {\n",
    "    'accuracy': 'accuracy',\n",
    "    'precision': 'precision',\n",
    "    'recall': 'recall',\n",
    "    'f1': 'f1',\n",
    "    'roc_auc': 'roc_auc',\n",
    "    'specificity': specificity_scorer\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hyperparameter Tuning\n",
    "\n",
    "`Optuna` was used in order to find the best hyperparameters for this model. An objective function was created to combine mean accuracy and mean AUC to evaluate the model's performance, and StratifiedKFold cross-validation to do the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'optuna' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 13\u001b[0m\n\u001b[0;32m      9\u001b[0m     mean_auc \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mmean(cv_results[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtest_roc_auc\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m     11\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;241m0.5\u001b[39m \u001b[38;5;241m*\u001b[39m mean_accuracy \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m0.5\u001b[39m \u001b[38;5;241m*\u001b[39m mean_auc\n\u001b[1;32m---> 13\u001b[0m sampler \u001b[38;5;241m=\u001b[39m \u001b[43moptuna\u001b[49m\u001b[38;5;241m.\u001b[39msamplers\u001b[38;5;241m.\u001b[39mTPESampler(seed\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m)\n\u001b[0;32m     14\u001b[0m study \u001b[38;5;241m=\u001b[39m optuna\u001b[38;5;241m.\u001b[39mcreate_study(direction\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmaximize\u001b[39m\u001b[38;5;124m'\u001b[39m, sampler\u001b[38;5;241m=\u001b[39msampler)\n\u001b[0;32m     15\u001b[0m study\u001b[38;5;241m.\u001b[39moptimize(objective, n_trials\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1000\u001b[39m, n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'optuna' is not defined"
     ]
    }
   ],
   "source": [
    "# ============================\n",
    "# Hyperparameter Tuning with Optuna\n",
    "# ============================\n",
    "\n",
    "def objective(trial):\n",
    "    model = create_model(trial)\n",
    "    cv_results = cross_validate(model, X, y, cv=skf, scoring=['accuracy', 'roc_auc'], n_jobs=-1)\n",
    "    mean_accuracy = np.mean(cv_results['test_accuracy'])\n",
    "    mean_auc = np.mean(cv_results['test_roc_auc'])\n",
    "\n",
    "    return 0.5 * mean_accuracy + 0.5 * mean_auc\n",
    "\n",
    "sampler = optuna.samplers.TPESampler(seed=42)\n",
    "study = optuna.create_study(direction='maximize', sampler=sampler)\n",
    "study.optimize(objective, n_trials=1000, n_jobs=-1)\n",
    "\n",
    "# Use the best hyperparameters to create the final model\n",
    "best_params = study.best_params\n",
    "print(\"Best Hyperparameters:\", best_params)\n",
    "print(\"Best Value:\", study.best_value)\n",
    "final_model = XGBClassifier(**best_params, random_state=42, eval_metric='logloss')\n",
    "#final_model = XGBClassifier(\n",
    "#    learning_rate=0.1,       # Step size shrinkage\n",
    "#    n_estimators=100,        # Number of trees\n",
    "#    max_depth=6,             # Maximum depth of trees\n",
    "#    min_child_weight=1,      # Minimum sum of instance weight (hessian) in a child\n",
    "#    subsample=0.8,           # Subsample ratio of training data\n",
    "#    colsample_bytree=0.8,    # Subsample ratio of features per tree\n",
    "#    gamma=0,                 # Minimum loss reduction to make a split\n",
    "#    reg_alpha=0,             # L1 regularization\n",
    "#    reg_lambda=1,            # L2 regularization\n",
    "#    scale_pos_weight=1,      # Used for imbalanced datasets; adjust if needed\n",
    "#    random_state=42,         # Ensures reproducibility\n",
    "#    verbosity=1              # Controls the output verbosity\n",
    "#)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Results\n",
    "\n",
    "Finally, we evaluated the model using cross-validation, obtaining important performance metrics and feature importance. The multiple performance metrics were displayed and ROC curves were plotted so we could analyse our results and see what could be improved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================\n",
    "# Final Evaluation\n",
    "# ============================\n",
    "cv_results = cross_validate(\n",
    "    final_model, X, y, cv=skf, scoring=scoring, return_estimator=True, n_jobs=-1\n",
    ")\n",
    "\n",
    "feature_importances = []\n",
    "for estimator in cv_results['estimator']:\n",
    "    feature_importances.append(estimator.feature_importances_)\n",
    "\n",
    "average_importances = np.mean(feature_importances, axis=0)\n",
    "\n",
    "feature_names = X.columns\n",
    "importance_df = pd.DataFrame({\n",
    "    'Feature': feature_names,\n",
    "    'Average Importance': average_importances\n",
    "}).sort_values(by='Average Importance', ascending=False)\n",
    "\n",
    "# Display the feature importance table\n",
    "display(importance_df)\n",
    "\n",
    "# Plot the feature importances\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.barh(importance_df['Feature'], importance_df['Average Importance'])\n",
    "plt.xlabel('Average Importance')\n",
    "plt.ylabel('Feature')\n",
    "plt.title('Average Feature Importance Across Folds')\n",
    "plt.gca().invert_yaxis()\n",
    "plt.show()\n",
    "\n",
    "print(\"Mean Cross-Validation Accuracy:\", cv_results['test_accuracy'].mean())\n",
    "print(\"Mean Cross-Validation Precision:\", cv_results['test_precision'].mean())\n",
    "print(\"Mean Cross-Validation Recall:\", cv_results['test_recall'].mean())\n",
    "print(\"Mean Cross-Validation F1 Score:\", cv_results['test_f1'].mean())\n",
    "print(\"Mean Cross-Validation AUC Score:\", cv_results['test_roc_auc'].mean())\n",
    "print(\"Mean Cross-Validation Specificity:\", cv_results['test_specificity'].mean())\n",
    "\n",
    "# ROC Curve Visualization with Individual Folds\n",
    "plt.figure(figsize=(10, 8))\n",
    "mean_fpr = np.linspace(0, 1, 100)\n",
    "tprs = []\n",
    "aucs = []\n",
    "for i, (train_index, test_index) in enumerate(skf.split(X, y)):\n",
    "    X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "    y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "    \n",
    "    final_model.fit(X_train, y_train)\n",
    "    y_proba = final_model.predict_proba(X_test)[:, 1]\n",
    "    \n",
    "    fpr, tpr, _ = roc_curve(y_test, y_proba)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    aucs.append(roc_auc)\n",
    "    tprs.append(np.interp(mean_fpr, fpr, tpr))\n",
    "    tprs[-1][0] = 0.0\n",
    "    \n",
    "    plt.plot(fpr, tpr, lw=1, alpha=0.3, label=f'Fold {i+1} (AUC = {roc_auc:.2f})')\n",
    "\n",
    "# Plot the mean ROC curve\n",
    "mean_tpr = np.mean(tprs, axis=0)\n",
    "mean_tpr[-1] = 1.0\n",
    "mean_auc = auc(mean_fpr, mean_tpr)\n",
    "plt.plot(mean_fpr, mean_tpr, color='b', label=f'Mean ROC (AUC = {mean_auc:.2f})', lw=2)\n",
    "\n",
    "# Plot the random chance line\n",
    "plt.plot([0, 1], [0, 1], linestyle='--', color='r', label='Random Chance')\n",
    "\n",
    "plt.xlabel('False Positive Rate (1 - Specificity)')\n",
    "plt.ylabel('True Positive Rate (Sensitivity)')\n",
    "plt.title('ROC Curves with Individual Folds and Mean')\n",
    "plt.legend(loc='lower right')\n",
    "plt.grid(alpha=0.3)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
